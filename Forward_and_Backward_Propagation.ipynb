{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1. Explain the concept of forward propagation in a neural network.\n",
        "### Answer :\n",
        "Forward Propagation in a Neural Network\n",
        "Concept:\n",
        "Forward propagation is the process of passing input data through a neural network to produce an output. It involves matrix multiplications, applying activation functions, and computing the final prediction.\n",
        "Steps in Forward Propagation:\n",
        "1.\tInput Layer: The input features are passed into the network.\n",
        "2.\tWeight Multiplication: Each neuron calculates a weighted sum of its inputs:\n",
        "\n",
        "    ***Z=W⋅X+b***\n",
        "\n",
        "3.\tActivation Function: The weighted sum is passed through an activation function to introduce non-linearity:\n",
        "\n",
        "    ***A=f(Z)A = f(Z)A=f(Z)***\n",
        "\n",
        "4.\tRepeat for Hidden Layers: The output from one layer serves as the input for the next layer.\n",
        "5.\tOutput Layer: Produces the final prediction using an appropriate activation function (e.g., Softmax for classification, Linear for regression).\n",
        "________________________________________\n",
        "## Question 2. What is the purpose of the activation function in forward propagation\n",
        "### Answer :\n",
        "Role of Activation Functions in Forward Propagation\n",
        "* Introduces non-linearity to the network.\n",
        "* Helps in learning complex patterns in data.\n",
        "* Prevents the model from behaving like a linear regression model.\n",
        "Examples:\n",
        "* ReLU: Used in hidden layers to handle vanishing gradients.\n",
        "* Sigmoid/Tanh: Used when probability-based outputs are required.\n",
        "* Softmax: Used in multi-class classification.\n",
        "________________________________________\n",
        "## Question 3. Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
        "### Answer :\n",
        "Backward Propagation (Backpropagation) Algorithm\n",
        "Purpose:\n",
        "Backpropagation optimizes weights in a neural network by calculating the error gradient and updating weights to minimize loss.\n",
        "Steps in Backpropagation:\n",
        "1.\tCompute Loss: Compare the network’s output with the actual label.\n",
        "2.\tCalculate Gradient: Compute the derivative of loss w.r.t. output using the chain rule.\n",
        "3.\tPropagate Error Backward:\n",
        "    * Compute gradient for each layer (starting from output to input).\n",
        "    * Adjust weights using Gradient Descent or Adam optimizer.\n",
        "4.\tUpdate Weights:\n",
        "\n",
        "    ***W = W−α (∂W/∂L)***\n",
        "​\n",
        "\n",
        " where α\\alphaα is the learning rate.\n",
        "________________________________________\n",
        "## Question 4. What is the purpose of the chain rule in backpropagation\n",
        "### Answer :\n",
        "Role of the Chain Rule in Backpropagation\n",
        "The chain rule is essential for computing gradients in deep networks. Since each layer depends on the previous layer, we use the chain rule to propagate gradients backward:\n",
        "\n",
        "***dW/dL = (dA/dL) × (dZ/dA) × (dW/dZ)***\n",
        "\n",
        "This allows us to efficiently compute gradients for deep networks without manually deriving derivatives for each layer.\n",
        "________________________________________\n",
        "## Question 5. Implement the forward propagation process for a simple neural network with one hidden layer using NumPy\n",
        "### Answer :\n",
        "\n"
      ],
      "metadata": {
        "id": "LOEOT-sz3YJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Initialize parameters\n",
        "np.random.seed(42)\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 2\n",
        "\n",
        "# Random weight initialization\n",
        "W1 = np.random.randn(hidden_size, input_size)\n",
        "b1 = np.zeros((hidden_size, 1))\n",
        "W2 = np.random.randn(output_size, hidden_size)\n",
        "b2 = np.zeros((output_size, 1))\n",
        "\n",
        "# Forward propagation function\n",
        "def forward_propagation(X):\n",
        "    # Input to hidden layer\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    # Hidden to output layer\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "\n",
        "    return A2\n",
        "\n",
        "# Example input (3 features, 1 sample)\n",
        "X = np.array([[0.5], [1.2], [-0.3]])\n",
        "\n",
        "# Perform forward propagation\n",
        "output = forward_propagation(X)\n",
        "print(\"Output of the network:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twnWT79_5YgU",
        "outputId": "829b2d2b-248d-4f14-e792-bdafb457c42e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network: [[0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXycEw1I5cSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}